model:
  resblock: "1"
  upsample_rates: [8, 8]
  upsample_kernel_sizes: [16, 16]
  upsample_initial_channel: 512
  resblock_kernel_sizes: [3, 7, 11]
  resblock_dilation_sizes: [[1, 3, 5], [1, 3, 5], [1, 3, 5]]
  depthwise_separable_convolutions:
    generator: False
  activation_function: "everyvoice.utils.original_hifigan_leaky_relu"
  istft_layer: True
  msd_layers: 3
  mpd_layers: [2, 3, 5, 7, 11]
training:
  generator_warmup_steps: 0
  gan_type: "original"
  optimizer:
    learning_rate: 1e-4
    eps: 1e-8
    weight_decay: 0.01
    betas: [0.9, 0.98]
    name: adamw
  wgan_clip_value: 0.01
  use_weighted_sampler: False
  batch_size: 16
  save_top_k_ckpts: 5
  ckpt_steps: null
  ckpt_epochs: 1
  max_epochs: 1000
  seed: 1234
  finetune_checkpoint: "./logs_and_checkpoints/LJ/base-vocoder/checkpoints/last.ckpt"
  finetune: true
  filelist: "./preprocessed/LJ/processed_filelist.psv"
  filelist_loader: "everyvoice.utils.generic_dict_loader"
  logger:
    name: "LJ"
    save_dir: "./logs_and_checkpoints"
    sub_dir: "everyvoice.utils.get_current_time"
    version: "lj-finetune"
  val_data_workers: 0
  train_data_workers: 4
preprocessing: "./config/lj/preprocessing.yaml"
